version: '3'
x-spark-common:
  &spark-common
  build: . # from Dockerfile in directory
  entrypoint: ["/bin/bash", "/app/start-cluster.sh"]
  environment:
    &spark-common-env
    SPARK_HOME: /opt/spark

services:
  jupyter-server: 
    context: ./jupyter_image
    stdin_open: true # docker run -i
    tty: true        # docker run -t
    ports: 
      - target: 8888  #container
        published: 8888  #host
        protocol: tcp
        mode: host
  spark-master:
    <<: *spark-common
    context: ./spark_image
    ports:
      - target: 8080 # web UI
        published: 8080
        protocol: tcp
        mode: host
      - target: 7077 # master port
        published: 7077
        protocol: tcp
        mode: host
    environment:
      <<: *spark-common-env
      SPARK_TYPE: master # variable for script, as the master in cluster
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
  spark-worker-1:
    <<: *spark-common
    context: ./spark_image
    depends_on:
      - spark-master
    ports:
      - target: 8081  # web UI
        published: 8081
        protocol: tcp
        mode: host
    environment:
      <<: *spark-common-env
      SPARK_MASTER: spark://spark-master:7077 # refers to service "spark-master"
      SPARK_WORKER_WEBUI_PORT: 8081
