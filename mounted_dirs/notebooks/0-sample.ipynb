{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c45bef5-b37c-47ef-9c88-96119c2065d6",
   "metadata": {},
   "source": [
    "# SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d8752-c3b3-4ef0-95e9-62867e09fa3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SparkSession with Python dependencies\n",
    "\n",
    "> + Resource Manager is buildin \"Standalone\"  \n",
    "> + Deploy Mode is \"client\" (only possible with interactive session)\n",
    "\n",
    "Starting a SparkSession with Python dependencies send by `--archives` flag\n",
    "\n",
    "+ N.B. these configs don't work as `os.environ`\n",
    "\n",
    "```python\n",
    "    .config(\"spark.archives\", \"/app/jobs/pyspark_venv.tar.gz#environment\")\\\n",
    "    .config(\"spark.pyspark.python\", \"./environment/bin/python\")\\\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982f3ca1-9f27-4558-be10-f5b1769e4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/yuan/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/09 15:59:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b5452884b516:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook-dep</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff5ac689ac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession \n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "# sets the Python path for workers, pointing to the extracted archive under a SparkSession's working directory\n",
    "# e.g. full path on worker is like: /opt/spark/work/app-20211009135519-0000/1/./environment\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"pyspark-notebook-dep\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.archives\", \"/app/jobs/pyspark_venv.tar.gz#environment\")\\\n",
    "    .getOrCreate()\n",
    "# spark.archvies add the file from this machine (jupyter-server) to the workers\n",
    "# it's extracted by Spark to subdirectory \"environment\" in SparkSession's working directory\n",
    "# note this is in Client mode, so Driver should have the same dependencies installed with running Python environment\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce9d8c2-2920-4bb1-8901-497d485dd141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.75661355, -0.83595055, -0.54290339,  0.83210849, -0.78727577]),\n",
       " array([ 0.95666722, -1.34126376, -0.68323051, -1.15742816, -0.03667599]),\n",
       " array([-0.66918965, -0.54477455, -0.34275965, -0.46614391, -1.07408784])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# requires numpy package"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74725236-e0ef-49d0-bffb-f309d19e32c4",
   "metadata": {},
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "SparkFiles.getRootDirectory()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2a9173f-6312-4ae0-8ed9-4e860582f346",
   "metadata": {},
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\") # right or wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf234c6-35a6-47a6-a845-96e113feedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f3d46-e93f-4ff1-8aa0-3e8f013b959f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bdcca47-5457-4a92-91c8-1599b90b4454",
   "metadata": {},
   "source": [
    "## SparSession with files (not working)\n",
    "\n",
    "> + Send files to worker by SparkFiles\n",
    "\n",
    "+ [pyspark.SparkContext.addFile](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.addFile.html)\n",
    "+ [Class SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html)\n",
    "+ [Help with Simple pyspark example on Dataproc](https://groups.google.com/g/cloud-dataproc-discuss/c/cubkWrjkk2g)\n",
    "\n",
    "Notes: \n",
    "\n",
    "1. `spark.sparkContext.addFile` just adds the given file to a temporary path on Driver (e.g. this jupyter instance). The path and copied file can be found by `SparkFiles.get(<filename>)`, or just go to its directory at `SparkFiles.getRootDirectory()`. When action is performed, the copied file will be added to the workers in their working directory, which can be seen in _stderr_ of worker's logs. \n",
    "2. The problem is that how the path is interpreted. On driver it is fine, but on workers if it's interpreted literally then find is not found. Since the actual path on workers are just in the working directory. The example from documentation does work, however call to `SparkFiles.get(<filename>)` is executed on the workers. So I think that returns the correct path to the added file. Some more details are given in the third link above. \n",
    "3. For now, the data files should be added through the mounted data directory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "191a6e5f-4dec-4da7-8c17-719fa8612ddd",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"pyspark-notebook-file\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a54e0aec-a0a5-4090-98dc-f01f14bf6d0e",
   "metadata": {},
   "source": [
    "from pyspark import SparkFiles\n",
    "SparkFiles.getRootDirectory()\n",
    "# this path is on Driver, not on master or workers, i.e. this jupyter-server instance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9204f8f-78fe-4eed-8172-4f587de1631f",
   "metadata": {},
   "source": [
    "spark.sparkContext.addFile(\"/app/data/books.csv\")\n",
    "# when called, this adds \"books.csv\" to the /tmp/... subdirectory on this machine"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0a23c02-ec0a-4cc0-9694-558a5b23223a",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open(SparkFiles.get(\"books.csv\")) as f: \n",
    "    result = f.readlines()\n",
    "result[:5]\n",
    "# this is just executing on the driver, so it's getting the local file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70a7354f-fa43-45aa-884a-3848d9817ac4",
   "metadata": {
    "tags": []
   },
   "source": [
    "spark.sparkContext.textFile(SparkFiles.get('books.csv')).collect()[:5]\n",
    "# worker is searching for the file at path for driver, so it's not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd0f1a-bc08-46c5-9090-41ee4ce3341a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760e959-c54c-440e-be6b-365cd018919b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cca0484-b644-4631-851a-d069b6406c86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SparkSession\n",
    "\n",
    "> + Resource Manager is buildin \"Standalone\"  \n",
    "> + Deploy Mode is \"client\" (only possible with interactive session)\n",
    "\n",
    "The Python dependencies must be alreay installed on all workers (and Jupyter server)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e60a9f8f-0eb3-4531-b6b6-cd21591ad88b",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c9fa671-b9ab-44b8-9d39-233c7b86b5bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# ModuleNotFoundError: No module named 'numpy' \n",
    "# module numpy not installed on workers, this can be resolved by doing `pip install numpy` on workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7c609-e0df-4373-88c9-d0b52d567fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bf92d-6080-4565-8b14-8afaf880b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653e97d7",
   "metadata": {},
   "source": [
    "## SparkSession local mode\n",
    "\n",
    "> + Scheduler and executore all on the same JVM, i.e. this jupyter server instance\n",
    "\n",
    "If the dependencies are already with Jupyter Server, then it'll work"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb0f916b-959e-4c55-b7f3-7d939ed000e8",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# local[4] to use 4 cores, local[*] to use all\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e3336a4-b88a-4c4e-8636-2130623562e3",
   "metadata": {},
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# this only need numpy on jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320acea-b90a-4224-b299-b750a1cfc60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b24f7-6737-42ed-87fc-bfcabd58eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
