{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212d8752-c3b3-4ef0-95e9-62867e09fa3e",
   "metadata": {},
   "source": [
    "# Spark cluste Standalone mode, with pakced Python dependencies\n",
    "\n",
    "> Scheduler is Spark build-in \"standalone\" \n",
    "\n",
    "The packed package is created with: \n",
    "\n",
    "```bash\n",
    "python -m venv pyspark_venv\n",
    "pyspark_venv/bin/pip install -r requirements.txt\n",
    "pyspark_venv/bin/venv-pack --force -p pyspark_venv/ -o mounted_dirs/jobs/pyspark_venv.tar.gz\n",
    "```\n",
    "\n",
    "See: \n",
    "+ https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html\n",
    "+ https://databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982f3ca1-9f27-4558-be10-f5b1769e4c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://639ecc42c812:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook-dep</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f14b53528e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession \n",
    "# os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "# this sets the Python on current instance, i.e. on jupyter server\n",
    "# since Python packages on jupyter server is installed system-wide, no need to set this \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"pyspark-notebook-dep\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.archives\", \"/app/jobs/pyspark_venv.tar.gz#environment\")\\\n",
    "    .config(\"spark.pyspark.python\", \"./environment/bin/python\")\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce9d8c2-2920-4bb1-8901-497d485dd141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.75661355, -0.83595055, -0.54290339,  0.83210849, -0.78727577]),\n",
       " array([ 0.95666722, -1.34126376, -0.68323051, -1.15742816, -0.03667599]),\n",
       " array([-0.66918965, -0.54477455, -0.34275965, -0.46614391, -1.07408784])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212aeab8-d07b-438d-9742-51c53e0840ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     v\n",
       "0   1   1.0\n",
       "1   1   2.0\n",
       "2   2   3.0\n",
       "3   2   5.0\n",
       "4   2  10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\")\n",
    ").toPandas() # require Pandas package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf234c6-35a6-47a6-a845-96e113feedce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51645c4-7b11-4aa2-be20-78bebe292b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e43b4c4a-e2e9-4687-9503-391428f33d09",
   "metadata": {},
   "source": [
    "# Spark cluste Standalone mode\n",
    "\n",
    "> Scheduler is Spark build-in \"standalone\" \n",
    "\n",
    "The Python dependencies must be alreay installed on all workers (and Jupyter server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24d6766-13fb-4d87-9068-6b9abaafca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://639ecc42c812:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f81560394c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bba800f-d4d5-44b9-aaf0-e9ad057520a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.75661355, -0.83595055, -0.54290339,  0.83210849, -0.78727577]),\n",
       " array([ 0.95666722, -1.34126376, -0.68323051, -1.15742816, -0.03667599]),\n",
       " array([-0.66918965, -0.54477455, -0.34275965, -0.46614391, -1.07408784])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# ModuleNotFoundError: No module named 'numpy' \n",
    "# module numpy not installed on workers, this can be resolved by doing `pip install numpy` on workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80ddf3-1152-45c0-b7c1-bbbc003603cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bf92d-6080-4565-8b14-8afaf880b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653e97d7",
   "metadata": {},
   "source": [
    "# Spark Local mode\n",
    "\n",
    "> Scheduler and executore all on the same JVM, i.e. this jupyter server instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e141745a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://639ecc42c812:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6597f844c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# local[4] to use 4 cores, local[*] to use all\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208b05f5-c018-4e86-850c-993973443ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.75661355, -0.83595055, -0.54290339,  0.83210849, -0.78727577]),\n",
       " array([ 0.95666722, -1.34126376, -0.68323051, -1.15742816, -0.03667599]),\n",
       " array([-0.66918965, -0.54477455, -0.34275965, -0.46614391, -1.07408784])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# this only need numpy on jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320acea-b90a-4224-b299-b750a1cfc60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b24f7-6737-42ed-87fc-bfcabd58eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
