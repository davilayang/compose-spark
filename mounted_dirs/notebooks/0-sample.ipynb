{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212d8752-c3b3-4ef0-95e9-62867e09fa3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark cluste Standalone mode with dependencies\n",
    "\n",
    "> Resource Manager is buildin \"Standalone\" and Deploy Mode is \"client\" (only possible with interactive session)\n",
    "\n",
    "Starting a SparkSession with Python dependencies send by `--archives` flag\n",
    "\n",
    "+ N.B. these configs don't work as `os.environ`\n",
    "\n",
    "```python\n",
    "    .config(\"spark.archives\", \"/app/jobs/pyspark_venv.tar.gz#environment\")\\\n",
    "    .config(\"spark.pyspark.python\", \"./environment/bin/python\")\\\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982f3ca1-9f27-4558-be10-f5b1769e4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/yuan/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/09 15:59:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b5452884b516:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook-dep</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff5ac689ac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession \n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "# sets the Python path for workers, pointing to the extracted archive under a SparkSession's working directory\n",
    "# e.g. full path on worker is like: /opt/spark/work/app-20211009135519-0000/1/./environment\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"pyspark-notebook-dep\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.archives\", \"/app/jobs/pyspark_venv.tar.gz#environment\")\\\n",
    "    .getOrCreate()\n",
    "# spark.archvies add the file from this machine (jupyter-server) to the workers\n",
    "# it's extracted by Spark to subdirectory \"environment\" in SparkSession's working directory\n",
    "# note this is in Client mode, so Driver should have the same dependencies installed with running Python environment\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce9d8c2-2920-4bb1-8901-497d485dd141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.75661355, -0.83595055, -0.54290339,  0.83210849, -0.78727577]),\n",
       " array([ 0.95666722, -1.34126376, -0.68323051, -1.15742816, -0.03667599]),\n",
       " array([-0.66918965, -0.54477455, -0.34275965, -0.46614391, -1.07408784])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# requires numpy package"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74725236-e0ef-49d0-bffb-f309d19e32c4",
   "metadata": {},
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "SparkFiles.getRootDirectory()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2a9173f-6312-4ae0-8ed9-4e860582f346",
   "metadata": {},
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\") # right or wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf234c6-35a6-47a6-a845-96e113feedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51645c4-7b11-4aa2-be20-78bebe292b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e43b4c4a-e2e9-4687-9503-391428f33d09",
   "metadata": {},
   "source": [
    "# Spark cluster Standalone mode\n",
    "\n",
    "> Resource Manager is buildin \"Standalone\" and Deploy Mode is \"client\" (only possible with interactive session)\n",
    "\n",
    "\n",
    "The Python dependencies must be alreay installed on all workers (and Jupyter server)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e60a9f8f-0eb3-4531-b6b6-cd21591ad88b",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c9fa671-b9ab-44b8-9d39-233c7b86b5bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# ModuleNotFoundError: No module named 'numpy' \n",
    "# module numpy not installed on workers, this can be resolved by doing `pip install numpy` on workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bf92d-6080-4565-8b14-8afaf880b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653e97d7",
   "metadata": {},
   "source": [
    "# Spark Local mode\n",
    "\n",
    "> Scheduler and executore all on the same JVM, i.e. this jupyter server instance\n",
    "\n",
    "If the dependencies are already with Jupyter Server, then it'll work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e141745a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://639ecc42c812:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6597f844c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# local[4] to use 4 cores, local[*] to use all\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208b05f5-c018-4e86-850c-993973443ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.75661355, -0.83595055, -0.54290339,  0.83210849, -0.78727577]),\n",
       " array([ 0.95666722, -1.34126376, -0.68323051, -1.15742816, -0.03667599]),\n",
       " array([-0.66918965, -0.54477455, -0.34275965, -0.46614391, -1.07408784])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "x = RandomRDDs.normalVectorRDD(\n",
    "    spark.sparkContext, \n",
    "    numRows=10000, \n",
    "    numCols=5, \n",
    "    numPartitions=20, \n",
    "    seed=42\n",
    ")\n",
    "x.collect()[:3]\n",
    "# this only need numpy on jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320acea-b90a-4224-b299-b750a1cfc60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b24f7-6737-42ed-87fc-bfcabd58eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
